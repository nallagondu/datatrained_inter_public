{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMPCxRR86oBNBvFi0O2YXSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nallagondu/datatrained_inter_public/blob/main/Census_Income_Phase3_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Description**\n",
        "This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization, Silicon Graphics).\n",
        "\n",
        " A set of reasonably clean records was extracted using the following conditions:\n",
        " ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)).\n",
        "\n",
        "  The prediction task is to determine whether a person makes over $50K a year.\n",
        "\n",
        "\n",
        "**Description of fnlwgt (final weight)**\n",
        "\n",
        "The weights on the Current Population Survey (CPS) files are controlled to independent estimates of the civilian non-institutional population of the US. These are prepared monthly for us by Population Division here at the Census **Bureau. We use 3 sets of controls. These are:**\n",
        "\n",
        "1.\tA single cell estimate of the population 16+ for each state.\n",
        "2.\tControls for Hispanic Origin by age and sex.\n",
        "3.\tControls by Race, age and sex.\n",
        "\n",
        "We use all three sets of controls in our weighting program and \"rake\" through them 6 times so that by the end we come back to all the controls we used. The term estimate refers to population totals derived from CPS by creating \"weighted tallies\" of any specified socio-economic characteristics of the population. People with similar demographic characteristics should have similar weights. There is one important caveat to remember about this statement. That is that since the CPS sample is actually a collection of 51 state samples, each with its own probability of selection, the statement only applies within state.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9zDcmI0SxvCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Link-**\n",
        "â€¢\thttps://github.com/FlipRoboTechnologies/ML_-Datasets/blob/main/Census%20Income/Census%20Income.csv\n"
      ],
      "metadata": {
        "id": "jLjJkm2kx4j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "j4ecC5T-kLWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "census_data_url = 'https://raw.githubusercontent.com/FlipRoboTechnologies/ML_-Datasets/main/Census%20Income/Census%20Income.csv'\n",
        "\n",
        "df_Census =pd.read_csv(census_data_url)\n",
        "\n",
        "df_Census.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "_4IAHpvlkYyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census.info()"
      ],
      "metadata": {
        "id": "sm1TG_awraBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census.describe()"
      ],
      "metadata": {
        "id": "ouJiP0SAr10d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as per the above  Describe statistic we can see there are no missing values ,even lets check the missing and duplicate values if any ."
      ],
      "metadata": {
        "id": "-xnyRAeisF2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census.isnull().sum()"
      ],
      "metadata": {
        "id": "-udsOj3CsWQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census.duplicated().sum()"
      ],
      "metadata": {
        "id": "QRDhTZwlsbdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#view the duplicated rows\n",
        "#Display the duplicated rows including the first occurrence\n",
        "\n",
        "\n",
        "duplicated_rows = df_Census[df_Census.duplicated()]\n",
        "duplicated_rows"
      ],
      "metadata": {
        "id": "7NjFoUNgsrer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census.shape"
      ],
      "metadata": {
        "id": "IMpUiBAAy6eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census_cleaned =df_Census.drop_duplicates(inplace=True)\n",
        "df_Census.shape"
      ],
      "metadata": {
        "id": "lQHYig6sy-7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census.isnull().sum()"
      ],
      "metadata": {
        "id": "C2sAQQXFWqIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census.duplicated().sum()"
      ],
      "metadata": {
        "id": "U0smXLD6W19K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see there is no duplicates and no missing values .now we can go for another step  like df_describe"
      ],
      "metadata": {
        "id": "jNn04MPXXCix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census.describe()"
      ],
      "metadata": {
        "id": "MYfWxTKQXbbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Statistics can help in understanding the distribution and central tendencies of the data which is useful for further analysis and decision making process .\n",
        "\n",
        "\n",
        "\n",
        "The fact that the majority of values for cpacity_gain  and Capacity_loss are 0 (zero)  .\n",
        "\n",
        "Indicates that the most individuals in the dataset did not report any capptal or losses .\n",
        "\n",
        "this could be due to servral reasons like :              \n",
        "\n",
        "#1.Econamic Behavior: many people do not enagage in activities that result in capital gains or losses ,such as trading stocks or selling assets\n",
        "\n",
        "\n",
        "#2.income level:\n",
        "#Individuals with lower incomes might not have the finacial capacity to invest in assets that generate capital gains ot losses\n",
        "\n",
        "#3.Tax reporting , dataset Composition and econimic conditions\n",
        "\n"
      ],
      "metadata": {
        "id": "ykmVbGToYwbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analyze Zero Values**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uoC3TRnKaaCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_capital_gain_count = df_Census[df_Census['Capital_gain'] == 0].shape[0]\n",
        "zero_capital_loss_count = df_Census[df_Census['Capital_loss'] == 0].shape[0]\n",
        "print(\"Number of rows with zero capital gain:\", zero_capital_gain_count)\n",
        "print(\"Number of rows with zero capital loss:\", zero_capital_loss_count)"
      ],
      "metadata": {
        "id": "Y2MPTQ0nakV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_rows = df_Census.shape[0]\n",
        "total_rows"
      ],
      "metadata": {
        "id": "HJP1Et_tbZ7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_capital_gain_count = df_Census[df_Census['Capital_gain'] == 0].shape[0]\n",
        "zero_capital_loss_count = df_Census[df_Census['Capital_loss'] == 0].shape[0]\n",
        "\n",
        "zero_capital_gain_percentage = (zero_capital_gain_count / total_rows) * 100\n",
        "zero_capital_loss_percentage = (zero_capital_loss_count / total_rows) * 100\n",
        "print(\"Percentage of rows with zero capital gain:\", zero_capital_gain_percentage)\n",
        "print(\"Percentage of rows with zero capital loss:\", zero_capital_loss_percentage)"
      ],
      "metadata": {
        "id": "bab7bQtkb7h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Percentage of rows with zero capital gain: 91.66769117285469\n",
        "#Percentage of rows with zero capital loss: 95.33132530120481"
      ],
      "metadata": {
        "id": "GnLdtzFmc0fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#additional analysis\n",
        "#correlation between final weight and capital gain and loss\n",
        "\n",
        "corr_gain = df_Census[['Fnlwgt', 'Capital_gain']].corr()\n",
        "\n",
        "\n",
        "print(f\"Correlation between final weight and capital gain:\\n{corr_gain}\")\n"
      ],
      "metadata": {
        "id": "bEHPGpXycvP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The correlation matrix you provided indicates a perfect negative correlation between Fnlwgt (final weight) and Capital_gain. This result is unusual and suggests that as Fnlwgt increases, Capital_gain\n",
        "\n",
        "Correlation between final weight and capital gain:\n",
        "\n",
        "                Fnlwgt  Capital_gain\n",
        "\n",
        "Fnlwgt         1.000000        0.000433\n",
        "\n",
        "Capital_gain   0.000433      1.000000\n"
      ],
      "metadata": {
        "id": "eiFPR2kzfOxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census"
      ],
      "metadata": {
        "id": "R1g4vtq0eYf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_loss = df_Census[['Fnlwgt','Capital_loss']].corr()\n",
        "print(f\"Correlation between final weight and capital loss:\\n{corr_loss}\")"
      ],
      "metadata": {
        "id": "GqgWvieOd5cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation matrix between** Fnlwgt** **(final weight) and Capital_loss** shows a very **weak negative correlation** (-0.010267), which suggests that there is **almost no linear relationship between these two variables**.\n",
        "\n",
        "This result is much more typical and expected compared to the perfect negative correlation we saw earlier between Fnlwgt and Capital_gain."
      ],
      "metadata": {
        "id": "6mspYvMqfmOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Census.hist(figsize=(10,10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mOJ0KDOSgHwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#standardize the numerical columns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "num_cols = df_Census.select_dtypes(include=['number'])\n",
        "scaler = StandardScaler()\n",
        "num_cols_scaled = scaler.fit_transform(num_cols)\n",
        "num_cols"
      ],
      "metadata": {
        "id": "3aNR1gVcgRrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#standardize the numerical columns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "num_cols = df_Census.select_dtypes(include=['number'])\n",
        "scaler = StandardScaler()\n",
        "df_Census[num_cols.columns] = scaler.fit_transform(num_cols)\n",
        "df_Census.head()"
      ],
      "metadata": {
        "id": "82XdaUa9h_rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encode the categorical features\n",
        "df_Census = pd.get_dummies(df_Census, columns=['Workclass', 'Education', 'Marital_status', 'Occupation', 'Relationship', 'Race', 'Sex', 'Native_country'])\n",
        "df_Census.head()\n"
      ],
      "metadata": {
        "id": "Ca55QSmZhxzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encode the target feature\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_Census_enc = df_Census.copy()\n",
        "df_Census_enc['Income'] = label_encoder.fit_transform(df_Census['Income'])\n",
        "df_Census_enc.head()"
      ],
      "metadata": {
        "id": "YzR6ry1jkKr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "sns.barplot(x='Education_num',y='Capital_gain',data=df_Census)\n",
        "plt.title('Education_num vs Capital_gain')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9hRU_yztlBK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "sns.barplot(y='Education_num',x='Income',data=df_Census)\n",
        "plt.title('Education_num vs Income')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xx4v1wDilnv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Correlation Matrix\n",
        "\n",
        "numerical_df_Census = df_Census .select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "corr_matrix = numerical_df_Census .corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix Census ')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PvUXs-9DmY7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per above correlation Graph\n",
        "1.age :  week negative correlation with Fnlwgt,weak positive correlation with Education,capital_gain,capital_loss and hours_per_week\n",
        "\n",
        "\n",
        "we can see most of them are week positive correlation and some are negitive correlation with each other  .\n",
        "\n"
      ],
      "metadata": {
        "id": "uNI7WsW4m6Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#analize\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.subplot(1,2,1)\n",
        "sns.histplot(df_Census,x = 'Capital_gain',hue='Income',multiple='stack', kde =True)\n",
        "plt.title('Capital_gain Distribution by Income ')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.histplot(data = df_Census,x='Capital_loss', hue='Income',multiple='stack', kde =True)\n",
        "plt.title('Capital_loss Distribution by Income')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IMzmJ5Winr7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "incom_analysis = df_Census.groupby('Income')[['Capital_gain','Capital_loss']].describe()\n",
        "incom_analysis"
      ],
      "metadata": {
        "id": "KWiKHqLlo0vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split the dataset into x and y\n",
        "y = df_Census_enc['Income']\n",
        "df_Census_enc.drop('Income', axis=1, inplace=True)\n",
        "X = df_Census_enc"
      ],
      "metadata": {
        "id": "EbOvsW6ZqGn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain a Logistic Regression Model using Coefficients**\n",
        "we will focus on explaining a logistic regression model.\n",
        "\n",
        " A logistic regression model is intrinsically interpretable because you can immediately explain the model by looking at the coefficients.\n",
        "\n",
        " Larger coefficients indicate a stronger influence on the target value.\n",
        "\n",
        " Furthermore, we can get both positive and negative coefficients, which positively and negatively influence the probability of the target.\n",
        "\n",
        "**We will do the following**\n",
        "\n",
        "Instantiate the logistic regression class and fit it to the data.\n",
        "\n",
        "To explain the model, you need to extract and plot the coefficients. For simplicity, only plot the top ten coefficients"
      ],
      "metadata": {
        "id": "ssCx_FHrqaP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "Lr_model = LogisticRegression()\n",
        "Lr_model.fit(X,y)"
      ],
      "metadata": {
        "id": "7sa2vMUhq1a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the Data\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "S7lwrtxsrbe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = Lr_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "62tSR31Mrux8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "#Classification Report\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "\n",
        "print(\"Coefficients:\\n\",Lr_model.coef_)"
      ],
      "metadata": {
        "id": "ENJX3_Qfrwox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the coefficients\n",
        "coefficients = Lr_model.coef_[0]\n",
        "#top_coefficients = coefficients.argsort()[-10:][::-1]\n",
        "\n",
        "feature_names = list(X.columns)\n",
        "coef_feature_pairs = list(zip(coefficients, feature_names))\n",
        "top_coefficients = sorted(coef_feature_pairs, key=lambda x: abs(x[0]), reverse=True)[:10]\n",
        "top_coefficients = [x[1] for x in top_coefficients]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(top_coefficients, [x[0] for x in top_coefficients])\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Top 10 Coefficients')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z3J2VduksykJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "roc_auc\n"
      ],
      "metadata": {
        "id": "xFyK0Rqrtyuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "Rf_model = RandomForestClassifier()\n",
        "Rf_model.fit(X_train,y_train)\n",
        "y_pred = Rf_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "importances = Rf_model.feature_importances_\n",
        "feature_imp_df =  pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
        "\n",
        "feature_imp_df = feature_imp_df.sort_values(by='Importance', ascending=False)\n",
        "top_n =10\n",
        "top_features = feature_imp_df.head(top_n)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(top_features['Feature'], top_features['Importance'])\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(f'Top {top_n} Features')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "X68LoZ0BuNXk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}